{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 410 Final Project: Generating Summaries for News Articles\n",
    "Aaron Kuhstoss, Shalin Mehta, and Aleksandra Grigortsuk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import random \n",
    "\n",
    "from rouge import Rouge\n",
    "from bert_score import score\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data Preprocessing\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86560\n"
     ]
    }
   ],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv(\"Latest_News.csv\")\n",
    "print(len(df))\n",
    "\n",
    "# Filtering dataset with English articles and non-NA \n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Subset df to make it workable, since python is computationally slow\n",
    "random.seed(410)\n",
    "df_subset = df.sample(n=15000)\n",
    "\n",
    "# Apply the language detection function to df\n",
    "df_subset['detected_language'] = df_subset['content'].apply(detect_language)\n",
    "english_articles = df_subset[df_subset['detected_language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207\n",
      "2268.2038111019056\n",
      "249.67170818505338\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>keywords</th>\n",
       "      <th>creator</th>\n",
       "      <th>video_url</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>full_description</th>\n",
       "      <th>image_url</th>\n",
       "      <th>source_id</th>\n",
       "      <th>detected_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28360</th>\n",
       "      <td>Where Inter And Juventus Are At After Derby D’...</td>\n",
       "      <td>https://www.forbes.com/sites/emmetgates/2021/1...</td>\n",
       "      <td>['SportsMoney', '/sportsmoney', 'Business', '/...</td>\n",
       "      <td>['Emmet Gates', ' Contributor']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both sides played out a cagey affair at San Si...</td>\n",
       "      <td>Both sides played out a cagey affair at San Si...</td>\n",
       "      <td>2021-10-25 14:51:31</td>\n",
       "      <td>Share to Facebook Share to Twitter Share to Li...</td>\n",
       "      <td>https://thumbor.forbes.com/thumbor/fit-in/0x0/...</td>\n",
       "      <td>forbes</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51727</th>\n",
       "      <td>Jubilant Pharmova Q2 Review - Radiopharma Busi...</td>\n",
       "      <td>https://www.bloombergquint.com/research-report...</td>\n",
       "      <td>['Jubilant Pharmova Ltd.', 'bqblue']</td>\n",
       "      <td>['Motilal Oswal Financial Services']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jubilant Pharmova Q2 Review - Radiopharma Busi...</td>\n",
       "      <td>BQ Blue’s special research section collates qu...</td>\n",
       "      <td>2021-10-25 05:42:18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bloombergquint</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63945</th>\n",
       "      <td>Joanna Cameron, Star of DC's The Secrets of Is...</td>\n",
       "      <td>https://comicbook.com/dc/news/joanna-camerona-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Adam Barnhardt']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Joanna Cameron, star of CBS' The Secrets of Is...</td>\n",
       "      <td>✖Joanna Cameron, star of CBS' The Secrets of I...</td>\n",
       "      <td>2021-10-24 20:50:00</td>\n",
       "      <td>✖ Joanna Cameron, star of CBS' The Secrets of ...</td>\n",
       "      <td>https://sportshub.cbsistatic.com/i/2021/10/24/...</td>\n",
       "      <td>comicbook</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81622</th>\n",
       "      <td>Parents more hesitant to vaccinate kids than t...</td>\n",
       "      <td>https://panow.com/2021/10/24/parents-more-hesi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OTTAWA - Jennifer Hubert jumped at the opportu...</td>\n",
       "      <td>OTTAWA — Jennifer Hubert jumped at the opportu...</td>\n",
       "      <td>2021-10-24 11:02:48</td>\n",
       "      <td>OTTAWA — Jennifer Hubert jumped at the opportu...</td>\n",
       "      <td>https://s3.amazonaws.com/socast-superdesk/medi...</td>\n",
       "      <td>panow</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30579</th>\n",
       "      <td>Salting routes at full capacity, meeting hears</td>\n",
       "      <td>https://kildare-nationalist.ie/2021/10/25/salt...</td>\n",
       "      <td>['Kildare News']</td>\n",
       "      <td>['Conor Forrest']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kildare Co Council has been asked to include t...</td>\n",
       "      <td>Kildare Co Council has been asked to include t...</td>\n",
       "      <td>2021-10-25 14:00:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://kildare-nationalist.ie/wp-content/uplo...</td>\n",
       "      <td>kildare-nationalist</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "28360  Where Inter And Juventus Are At After Derby D’...   \n",
       "51727  Jubilant Pharmova Q2 Review - Radiopharma Busi...   \n",
       "63945  Joanna Cameron, Star of DC's The Secrets of Is...   \n",
       "81622  Parents more hesitant to vaccinate kids than t...   \n",
       "30579     Salting routes at full capacity, meeting hears   \n",
       "\n",
       "                                                    link  \\\n",
       "28360  https://www.forbes.com/sites/emmetgates/2021/1...   \n",
       "51727  https://www.bloombergquint.com/research-report...   \n",
       "63945  https://comicbook.com/dc/news/joanna-camerona-...   \n",
       "81622  https://panow.com/2021/10/24/parents-more-hesi...   \n",
       "30579  https://kildare-nationalist.ie/2021/10/25/salt...   \n",
       "\n",
       "                                                keywords  \\\n",
       "28360  ['SportsMoney', '/sportsmoney', 'Business', '/...   \n",
       "51727               ['Jubilant Pharmova Ltd.', 'bqblue']   \n",
       "63945                                                NaN   \n",
       "81622                                                NaN   \n",
       "30579                                   ['Kildare News']   \n",
       "\n",
       "                                    creator video_url  \\\n",
       "28360       ['Emmet Gates', ' Contributor']       NaN   \n",
       "51727  ['Motilal Oswal Financial Services']       NaN   \n",
       "63945                    ['Adam Barnhardt']       NaN   \n",
       "81622                                   NaN       NaN   \n",
       "30579                     ['Conor Forrest']       NaN   \n",
       "\n",
       "                                             description  \\\n",
       "28360  Both sides played out a cagey affair at San Si...   \n",
       "51727  Jubilant Pharmova Q2 Review - Radiopharma Busi...   \n",
       "63945  Joanna Cameron, star of CBS' The Secrets of Is...   \n",
       "81622  OTTAWA - Jennifer Hubert jumped at the opportu...   \n",
       "30579  Kildare Co Council has been asked to include t...   \n",
       "\n",
       "                                                 content              pubDate  \\\n",
       "28360  Both sides played out a cagey affair at San Si...  2021-10-25 14:51:31   \n",
       "51727  BQ Blue’s special research section collates qu...  2021-10-25 05:42:18   \n",
       "63945  ✖Joanna Cameron, star of CBS' The Secrets of I...  2021-10-24 20:50:00   \n",
       "81622  OTTAWA — Jennifer Hubert jumped at the opportu...  2021-10-24 11:02:48   \n",
       "30579  Kildare Co Council has been asked to include t...  2021-10-25 14:00:54   \n",
       "\n",
       "                                        full_description  \\\n",
       "28360  Share to Facebook Share to Twitter Share to Li...   \n",
       "51727                                                NaN   \n",
       "63945  ✖ Joanna Cameron, star of CBS' The Secrets of ...   \n",
       "81622  OTTAWA — Jennifer Hubert jumped at the opportu...   \n",
       "30579                                                NaN   \n",
       "\n",
       "                                               image_url            source_id  \\\n",
       "28360  https://thumbor.forbes.com/thumbor/fit-in/0x0/...               forbes   \n",
       "51727                                                NaN       bloombergquint   \n",
       "63945  https://sportshub.cbsistatic.com/i/2021/10/24/...            comicbook   \n",
       "81622  https://s3.amazonaws.com/socast-superdesk/medi...                panow   \n",
       "30579  https://kildare-nationalist.ie/wp-content/uplo...  kildare-nationalist   \n",
       "\n",
       "      detected_language  \n",
       "28360                en  \n",
       "51727                en  \n",
       "63945                en  \n",
       "81622                en  \n",
       "30579                en  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the total number of articles in the 'english_articles' DataFrame\n",
    "print(len(english_articles))\n",
    "\n",
    "# Calculate and print the average length of the 'content' field in the 'english_articles' DataFrame\n",
    "print(english_articles['content'].str.len().mean())\n",
    "\n",
    "# Calculate and print the average length of the 'description' field in the 'english_articles' DataFrame\n",
    "print(english_articles['description'].str.len().mean())\n",
    "\n",
    "# Count and print the number of missing (null) values in the 'content' field of the 'english_articles' DataFrame\n",
    "print(english_articles['content'].isnull().sum())\n",
    "\n",
    "# Count and print the number of missing (null) values in the 'title' field of the 'english_articles' DataFrame\n",
    "print(english_articles['title'].isnull().sum())\n",
    "\n",
    "# Display the first few rows of the 'english_articles' DataFrame to get a quick overview of the data\n",
    "(english_articles.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Construction\n",
    "##### Summarization pipeline\n",
    "\n",
    "A detailed description of what the function does and how it achives its summaries is found in the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, per):\n",
    "    # Load the English language model from Spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Process the input text and tokenize it\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Initialize a dictionary to hold word frequencies\n",
    "    word_frequencies = {}\n",
    "\n",
    "    # Calculate the frequency of each word in the text, excluding stopwords and punctuation\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "    # Normalize word frequencies by dividing each by the maximum frequency\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    # Break the text into sentences\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "\n",
    "    # Score sentences based on the frequency of the words they contain\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    # Determine the number of sentences to include in the summary\n",
    "    select_length = int(len(sentence_tokens) * per)\n",
    "\n",
    "    # Select the sentences with the highest scores\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    # Combine the selected sentences into a final summary\n",
    "    final_summary = [word.text for word in summary]\n",
    "    summary = ''.join(final_summary)\n",
    "\n",
    "    # Return the summarized text\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Summaries\n",
    "\n",
    "The first cell directly below prints out 10 summaries from the summaries list using our algorithm. It tends to be a little time consuming (taking about 10 nminutes), but does indeed create the summaries for each article in our pre-preprocessed data. \n",
    "\n",
    "The second cell selectively generates summaries and stores them along with the corresponding articles but only does so for 5 articles starting from the 500th article. This makes the generation time alot quicker, and is what we use in our model evaluation.\n",
    "\n",
    "Both cells use a summary ratio of 0.1 for the creation of the summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Iterate over each article in the 'english_articles' DataFrame\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(english_articles)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Generate a summary of the article's content, summarized to 10% of its original length\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     summary \u001b[39m=\u001b[39m summarize(english_articles\u001b[39m.\u001b[39;49miloc[i,\u001b[39m6\u001b[39;49m], \u001b[39m0.1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# If a summary is successfully generated, add it to the summaries list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m summary:\n",
      "\u001b[1;32m/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Calculate the frequency of each word in the text, excluding stopwords and punctuation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m doc:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(STOP_WORDS):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m         \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m punctuation:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/shali/Documents/410NewsSummarizer-2/410_news_summaries.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m             \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mtext \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m word_frequencies\u001b[39m.\u001b[39mkeys():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the generated summaries\n",
    "summaries_1 = []\n",
    "\n",
    "# Iterate over each article in the 'english_articles' DataFrame\n",
    "for i in range(len(english_articles)):\n",
    "    # Generate a summary of the article's content, summarized to 10% of its original length\n",
    "    summary = summarize(english_articles.iloc[i,6], 0.1)\n",
    "\n",
    "    # If a summary is successfully generated, add it to the summaries list\n",
    "    if summary:\n",
    "        summaries_1.append(summary)\n",
    "\n",
    "# Retrieve and display the first 10 summaries from the generated list\n",
    "summaries_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store generated summaries and corresponding raw articles\n",
    "summaries_2 = []\n",
    "raw_articles = []\n",
    "\n",
    "# Start at the 500th article in the 'english_articles' DataFrame\n",
    "index = 500\n",
    "\n",
    "# Continue generating summaries until we have 5 of them\n",
    "while len(summaries_2) < 5:\n",
    "    # Retrieve the article at the current index\n",
    "    article = english_articles.iloc[index,:]\n",
    "    # Extract the content of the article\n",
    "    article_content = english_articles.iloc[index,6]\n",
    "\n",
    "    # Generate a summary of the article content, summarized to 10% of its original length\n",
    "    summary = summarize(article_content, 0.1)\n",
    "\n",
    "    # If a summary is successfully generated, add it to the summaries list and the corresponding article to the raw articles list\n",
    "    if summary:\n",
    "        summaries_2.append(summary)\n",
    "        raw_articles.append(article)\n",
    "\n",
    "    # Move to the next article\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: A fundamental issue with using ROUGE and BLEU metrics is their dependence on high-quality reference summaries.\n",
    "Using the \"description\" field as a pseudo summary provides a good amount of reference data. However, the quality of these descriptions varies, and some are null.\n",
    "Another approach is to write summaries by hand for comparison, but this is time-consuming and limits the data points.\n",
    "\"\"\"\n",
    "\n",
    "# Obtain original descriptions for use as reference \"summaries\"\n",
    "descriptions = [row.iloc[5] for row in raw_articles]\n",
    "\n",
    "# Get raw content of articles for BERTscoring\n",
    "orig_articles = [row.iloc[6] for row in raw_articles]\n",
    "\n",
    "# Uncomment the below lines to display raw article content and descriptions\n",
    "# print('Raw Articles:', '\\n'.join(orig_articles))\n",
    "# print('Descriptions:', '\\n'.join(descriptions))\n",
    "\n",
    "# Uncomment the below line to display summaries for qualitative evaluation\n",
    "# print('Summaries:', '\\n'.join(summaries))\n",
    "\n",
    "# Hand-written summaries for evaluation\n",
    "own_summaries = [\n",
    "    \"Bryan Cranstons portrayal of the morally gray character Walter White in the hit series Breaking Bad has been loved by fans. However, AMC had other choices for the lead, with Breaking Bad creator Vince Gilligan ultimately persuading executives to choose Cranston.\",\n",
    "    \"Recently on TikTok, the term weaponized incompetence is gaining a lot of attention. According to psychotherapist and writer, Emily Mendez, M.S. EdS, “Weaponized incompetence refers to pretending not to know how to do something when you do really know how to do it.” The term has 21.8M views on TikTok as example, mostly of women, whose colleagues, partners, and family members use weaponized incompetence to get out of work.\",\n",
    "    \"The All India Congress is going to launch a country wide protest from November 14 against the abnormal rise of fuel. The massive protest against the high fuel price will start from November 14 and will continue till November 29, after five consecutive days of rising fuel prices across the country.\",\n",
    "    \"Numerous artists across multiple genres, such as Lil Nas X, Ariana Grande, and Olivia Rodrigo are entering songs for consideration in the upcoming Grammy award season. This includes Justin Bieber, whose smash hit “Peaches” (featuring Daniel Caesar and Giveon) is vying for a Grammy nomination as best R&B performance.\",\n",
    "    \"Singer-songwriter Ed Sheeran announced Sunday he had tested positive for COVID-19 and would be self-isolating in his home five days before he is scheduled to release his fourth studio album. Sheeran's upcoming album, titled '=' is scheduled to be released on October 29.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To see the evaluation metrics we have for each of the 5 articles using BERT and ROUGE, please select the \"scrollable element\" option in the output below (because it is truncated by default)\n",
    "\n",
    "##### We decided to compare our summaries using both metrics on our own hand-written samples as well as the description given in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Evaluation Against Hand-Written Summaries =====\n",
      "--- Summary 1 (Compared with Hand-Written) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.0541\n",
      "    P: 0.1053\n",
      "    F: 0.0714\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.0541\n",
      "    P: 0.1053\n",
      "    F: 0.0714\n",
      "\n",
      "--- Summary 2 (Compared with Hand-Written) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.0909\n",
      "    P: 0.1250\n",
      "    F: 0.1053\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.0909\n",
      "    P: 0.1250\n",
      "    F: 0.1053\n",
      "\n",
      "--- Summary 3 (Compared with Hand-Written) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.1622\n",
      "    P: 0.0938\n",
      "    F: 0.1188\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.1351\n",
      "    P: 0.0781\n",
      "    F: 0.0990\n",
      "\n",
      "--- Summary 4 (Compared with Hand-Written) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.1333\n",
      "    P: 0.0938\n",
      "    F: 0.1101\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.1111\n",
      "    P: 0.0781\n",
      "    F: 0.0917\n",
      "\n",
      "--- Summary 5 (Compared with Hand-Written) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.0541\n",
      "    P: 0.0952\n",
      "    F: 0.0690\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.0541\n",
      "    P: 0.0952\n",
      "    F: 0.0690\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTscore Results:\n",
      "Summary 1 (Compared with Hand-Written):\n",
      "  Precision: 0.9336, Recall: 0.8083, F1: 0.8665\n",
      "\n",
      "Summary 2 (Compared with Hand-Written):\n",
      "  Precision: 0.9676, Recall: 0.8141, F1: 0.8842\n",
      "\n",
      "Summary 3 (Compared with Hand-Written):\n",
      "  Precision: 0.8631, Recall: 0.7871, F1: 0.8233\n",
      "\n",
      "Summary 4 (Compared with Hand-Written):\n",
      "  Precision: 0.9586, Recall: 0.8104, F1: 0.8783\n",
      "\n",
      "Summary 5 (Compared with Hand-Written):\n",
      "  Precision: 0.9295, Recall: 0.7889, F1: 0.8534\n",
      "\n",
      "\n",
      "===== Evaluation Against Article Descriptions =====\n",
      "--- Summary 1 (Compared with Article Description) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.2857\n",
      "    P: 0.2105\n",
      "    F: 0.2424\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.2143\n",
      "    P: 0.1579\n",
      "    F: 0.1818\n",
      "\n",
      "--- Summary 2 (Compared with Article Description) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.8298\n",
      "    P: 0.9750\n",
      "    F: 0.8966\n",
      "  ROUGE-2:\n",
      "    R: 0.8302\n",
      "    P: 0.9778\n",
      "    F: 0.8980\n",
      "  ROUGE-L:\n",
      "    R: 0.8298\n",
      "    P: 0.9750\n",
      "    F: 0.8966\n",
      "\n",
      "--- Summary 3 (Compared with Article Description) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.4000\n",
      "    P: 0.0312\n",
      "    F: 0.0580\n",
      "  ROUGE-2:\n",
      "    R: 0.0000\n",
      "    P: 0.0000\n",
      "    F: 0.0000\n",
      "  ROUGE-L:\n",
      "    R: 0.4000\n",
      "    P: 0.0312\n",
      "    F: 0.0580\n",
      "\n",
      "--- Summary 4 (Compared with Article Description) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.2500\n",
      "    P: 0.1406\n",
      "    F: 0.1800\n",
      "  ROUGE-2:\n",
      "    R: 0.0256\n",
      "    P: 0.0139\n",
      "    F: 0.0180\n",
      "  ROUGE-L:\n",
      "    R: 0.1944\n",
      "    P: 0.1094\n",
      "    F: 0.1400\n",
      "\n",
      "--- Summary 5 (Compared with Article Description) ---\n",
      "ROUGE Scores:\n",
      "  ROUGE-1:\n",
      "    R: 0.2941\n",
      "    P: 0.2381\n",
      "    F: 0.2632\n",
      "  ROUGE-2:\n",
      "    R: 0.0625\n",
      "    P: 0.0385\n",
      "    F: 0.0476\n",
      "  ROUGE-L:\n",
      "    R: 0.1765\n",
      "    P: 0.1429\n",
      "    F: 0.1579\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTscore Results:\n",
      "Summary 1 (Compared with Article Description):\n",
      "  Precision: 0.9336, Recall: 0.8083, F1: 0.8665\n",
      "\n",
      "Summary 2 (Compared with Article Description):\n",
      "  Precision: 0.9676, Recall: 0.8141, F1: 0.8842\n",
      "\n",
      "Summary 3 (Compared with Article Description):\n",
      "  Precision: 0.8631, Recall: 0.7871, F1: 0.8233\n",
      "\n",
      "Summary 4 (Compared with Article Description):\n",
      "  Precision: 0.9586, Recall: 0.8104, F1: 0.8783\n",
      "\n",
      "Summary 5 (Compared with Article Description):\n",
      "  Precision: 0.9295, Recall: 0.7889, F1: 0.8534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate summaries using Rouge and BERTscores\n",
    "def eval_summaries(generated_summaries, hand_written_summaries, descriptions, content):\n",
    "    \"\"\"\n",
    "    This function evaluates the generated summaries using ROUGE and BERTscores,\n",
    "    comparing them with hand-written summaries and article descriptions.\n",
    "    It prints the precision, recall, and F1 scores for each summary,\n",
    "    indicating the type of reference used.\n",
    "    \"\"\"\n",
    "    # Evaluate against hand-written summaries\n",
    "    print(\"===== Evaluation Against Hand-Written Summaries =====\")\n",
    "    _evaluate_each_summary(generated_summaries, hand_written_summaries, content, \"Hand-Written\")\n",
    "\n",
    "    # Evaluate against article descriptions\n",
    "    print(\"\\n===== Evaluation Against Article Descriptions =====\")\n",
    "    _evaluate_each_summary(generated_summaries, descriptions, content, \"Article Description\")\n",
    "\n",
    "def _evaluate_each_summary(summaries, references, content, reference_type):\n",
    "    # Initialize the ROUGE object\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(summaries, references)\n",
    "\n",
    "    # Displaying ROUGE scores\n",
    "    for index, score_set in enumerate(rouge_scores):\n",
    "        print(f\"--- Summary {index + 1} (Compared with {reference_type}) ---\")\n",
    "        print(\"ROUGE Scores:\")\n",
    "        for rouge_key, values in score_set.items():\n",
    "            print(f\"  {rouge_key.upper()}:\")\n",
    "            for metric, value in values.items():\n",
    "                print(f\"    {metric.capitalize()}: {value:.4f}\")\n",
    "        print()\n",
    "\n",
    "    # Get BERTscore\n",
    "    P, R, F1 = score(summaries, content, lang='en')\n",
    "\n",
    "    # Display BERTscore results\n",
    "    print(\"BERTscore Results:\")\n",
    "    for i in range(len(summaries)):\n",
    "        print(f\"Summary {i+1} (Compared with {reference_type}):\")\n",
    "        print(f\"  Precision: {P[i].item():.4f}, Recall: {R[i].item():.4f}, F1: {F1[i].item():.4f}\")\n",
    "        print()\n",
    "\n",
    "# Evaluating generated summaries\n",
    "eval_summaries(summaries_2, own_summaries, descriptions, orig_articles)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
